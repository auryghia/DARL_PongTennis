{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5083e522",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a24878f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inizio test...\n",
      "Spazio di osservazione dell'ambiente di test wrappato: Box(0, 255, (210, 160, 3), uint8)\n",
      "GPU not available, testing on CPU.\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "Modello caricato da: checkpoints_\\pong_a2c_ckpt_300000_steps.zip\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "from stable_baselines3 import A2C\n",
    "import torch\n",
    "import gymnasium as gym\n",
    "from ale_py import ALEInterface\n",
    "import ale_py\n",
    "import sys\n",
    "from stable_baselines3.common.atari_wrappers import AtariWrapper\n",
    "\n",
    "import os\n",
    "\n",
    "gym.register_envs(ale_py)\n",
    "\n",
    "ale = ALEInterface()\n",
    "env = gym.make(\"ALE/Pong-v5\")\n",
    "\n",
    "def test_agent(model_path, env, num_episodes=10, render=True, use_gpu=True):\n",
    "    \"\"\"\n",
    "    Testa un agente addestrato.\n",
    "\n",
    "    :param model_path: Percorso del modello salvato (.zip).\n",
    "    :param env: L'ambiente di test (dovrebbe essere lo stesso tipo di ambiente usato per l'addestramento).\n",
    "    :param num_episodes: Numero di episodi per cui testare l'agente.\n",
    "    :param render: Se True, renderizza l'ambiente durante il test.\n",
    "    :param use_gpu: Se True e la GPU è disponibile, carica il modello sulla GPU.\n",
    "    \"\"\"\n",
    "    # Determina il device\n",
    "    if use_gpu and torch.cuda.is_available():\n",
    "        current_device = torch.device(\"cuda\")\n",
    "        print(\"Testing on GPU.\")\n",
    "    else:\n",
    "        if use_gpu and not torch.cuda.is_available():\n",
    "            print(\"GPU not available, testing on CPU.\")\n",
    "        else:\n",
    "            print(\"Testing on CPU.\")\n",
    "        current_device = torch.device(\"cpu\")\n",
    "\n",
    "    # Carica il modello addestrato\n",
    "    # Assicurati che l'ambiente passato a A2C.load() sia un'istanza dell'ambiente\n",
    "    # o None se l'ambiente è già wrappato e non vuoi che SB3 lo wrappi di nuovo.\n",
    "    # Per coerenza, è meglio passare l'ambiente.\n",
    "\n",
    "    wrap_env =   AtariWrapper(env) # Aggiungi altri parametri se usati in addestramento\n",
    "\n",
    "    try:\n",
    "        model = A2C.load(model_path, env=wrap_env, device=current_device)\n",
    "        print(f\"Modello caricato da: {model_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Errore durante il caricamento del modello: {e}\")\n",
    "        return\n",
    "\n",
    "    total_rewards = []\n",
    "    total_steps = []\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        obs, _ = wrap_env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        episode_steps = 0\n",
    "        max_steps = 1000  # Limita il numero massimo di passi per episodio\n",
    "        while not done and episode_steps < max_steps:\n",
    "            # Usa deterministic=True per ottenere l'azione più probabile (comportamento di exploitation)\n",
    "            action, _states = model.predict(obs, deterministic=True)\n",
    "            obs, reward, done, _, info = wrap_env.step(action)\n",
    "            episode_reward += reward\n",
    "            episode_steps += 1\n",
    "            time.sleep(0.01)  # Rallenta un po' per la visualizzazione\n",
    "            if render:\n",
    "                wrap_env.render()  # Usa il metodo render dell'ambiente wrappato\n",
    "\n",
    "        if episode_steps >= max_steps:\n",
    "            print(f\"Raggiunto il limite massimo di passi ({max_steps}) per l'episodio.\")\n",
    "\n",
    "        total_rewards.append(episode_reward)\n",
    "        total_steps.append(episode_steps)\n",
    "        print(f\"Episodio {episode + 1}: Ricompensa = {episode_reward}, Steps = {episode_steps}\")\n",
    "\n",
    "    env.close() # Chiudi l'ambiente dopo il test\n",
    "    avg_reward = np.mean(total_rewards)\n",
    "    std_reward = np.std(total_rewards)\n",
    "    avg_steps = np.mean(total_steps)\n",
    "    print(f\"\\n--- Risultati del Test ---\")\n",
    "    print(f\"Numero di episodi: {num_episodes}\")\n",
    "    print(f\"Ricompensa media: {avg_reward:.2f} +/- {std_reward:.2f}\")\n",
    "    print(f\"Durata media episodio (steps): {avg_steps:.2f}\")\n",
    "    print(f\"--------------------------\")\n",
    "    return avg_reward, std_reward\n",
    "\n",
    "# ... (resto del codice, inclusa la funzione collect_observations) ...\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Esempio di come potresti eseguire l'addestramento e poi il test\n",
    "    \n",
    "    # 1. Crea l'ambiente\n",
    "    # env_train = make_env()\n",
    "    # env_train = DummyVecEnv([lambda: env_train]) # SB3 si aspetta un VecEnv\n",
    "\n",
    "    # 2. Addestra l'agente (o salta se hai già un modello)\n",
    "    # print(\"Inizio addestramento...\")\n",
    "    # trained_model_instance = train_agent(env_train, total_timesteps=200000, checkpoint_interval=50000, use_gpu=True)\n",
    "    # path_to_trained_model = \"checkpoints/pong_a2c.zip_checkpoint_200000.zip\" # Assicurati che il percorso sia corretto\n",
    "    # env_train.close()\n",
    "    # print(\"Addestramento completato.\")\n",
    "\n",
    "    # 3. Testa l'agente\n",
    "    print(\"\\nInizio test...\")\n",
    "    model_path = \"checkpoints_\\pong_a2c_ckpt_300000_steps.zip\"\n",
    "                                                                        # Dovrebbe essere il percorso esatto del tuo file .zip\n",
    "                                                                        # ad esempio, se save_path era \"checkpoints/pong_a2c\"\n",
    "                                                                        # e checkpoint_interval era 100000,\n",
    "                                                                        # il file per 200000 timesteps potrebbe essere\n",
    "                                                                        # \"checkpoints/pong_a2c_checkpoint_200000.zip\"\n",
    "                                                                        # o \"checkpoints/pong_a2c.zip_checkpoint_200000.zip\"\n",
    "                                                                        # a seconda di come hai costruito il nome.\n",
    "                                                                        # Controlla la tua cartella checkpoints!\n",
    "\n",
    "\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"ERRORE: Il file del modello {model_path} non esiste. Controlla il percorso.\")\n",
    "\n",
    "    else:\n",
    "        # Non è necessario wrappare in DummyVecEnv per A2C.load se passi l'ambiente singolo\n",
    "        # Tuttavia, se A2C.load si aspetta un VecEnv, allora wrappalo:\n",
    "        # env_test = DummyVecEnv([lambda: env_test])\n",
    "\n",
    "        ale = ALEInterface()\n",
    "        env = gym.make(\"ALE/Pong-v5\", render_mode='human' if True else None)\n",
    "        print(f\"Spazio di osservazione dell'ambiente di test wrappato: {env.observation_space}\")\n",
    "        test_agent(model_path, env, num_episodes=5, render=True, use_gpu=True)\n",
    "        test_agent(model_path, env, num_episodes=20, render=True, use_gpu=True)\n",
    "        # env_test.close() # test_agent ora chiude l'ambiente\n",
    "\n",
    "        env.close() # Chiudi l'ambiente dopo il test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
